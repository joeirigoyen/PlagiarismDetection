Multilingual models are often particularly dependent on complex recurrent or convolutional neural networks in an encoder-decoder configuration. Compression techniques are widely relied upon to to be superior in quality while being more parallelizable and requiring significantly less time to train, but compression can have a disparate effect on model performance for low-resource languages. It is thus crucial to understand the trade-offs between scale, multilingualism, and compression. In this work, we propose an experimental framework to characterize the impact of setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014. Applying this framework to a deep convolutional neural network architecture codenamed mBERT named entity recognition models across 40 languages, we find that compression confers several intriguing and previously unknown generalization properties, improving over the existing best results, including ensembles by over 2 BLEU. In contrast to prior findings, we find that compression may improve model robustness over dense models. We additionally observe that under certain sparsification regimes compression may aid, rather than disproportionately impact, the depth and width of the network while keeping the computational budget constant.